{"cells": [{"cell_type": "markdown", "id": "c7e50351-15c7-4379-9369-cd41cd7ac272", "metadata": {}, "source": ["# (Homework) Week 6 - DataScience Bootcamp Fall 2025\n", "\n", "All solution cells are replaced with `# TODO` placeholders so you can fill them in.\n", "\n", "**Name:** \\\n", "**Email:**\n", "\n", "---"]}, {"cell_type": "markdown", "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8", "metadata": {}, "source": ["### Problem 1: Dataset Splitting\n", "\n", "1. You have recordings of 44 phones from 100 people; each person records ~200 phones/day for 5 days.\n", "   - Design a valid training/validation/test split strategy that ensures the model generalizes to **new speakers**.\n", "\n", "2. You now receive an additional dataset of 10,000 phone recordings from **Kilian**, a single speaker.\n", "   - You must train a model that performs well **specifically for Kilian**, while also maintaining generalization.\n", "\n", "*Describe your proposed split strategy and reasoning.* (Theory)"]}, {"cell_type": "code", "execution_count": null, "id": "a65cfdb6-aca2-4dd7-aaa4-70fa30af475e", "metadata": {}, "outputs": [], "source": ["\"\"\"Problem 1 \u2013 Dataset Splitting\n", "\n", "(1) How I would split the original 100-speaker dataset\n", "\n", "To evaluate generalization to *new* speakers, we must split the data at the **speaker level**, not at the recording level.\n", "\n", "A simple speaker-level split:\n", "- Train: 70 speakers\n", "- Validation: 15 speakers\n", "- Test: 15 speakers\n", "\n", "For each speaker, all of their recordings (all 5 days and all phones) are placed into exactly one split. No speaker appears in more than one split. This way, test accuracy reflects how well the model generalizes to completely unseen speakers.\n", "\n", "(2) How to handle Kilian\u2019s 10,000 additional recordings\n", "\n", "Now we have:\n", "- The original 100-speaker dataset.\n", "- A large single-speaker dataset from Kilian (10,000 recordings).\n", "\n", "We care about:\n", "- Performance on \u201caverage\u201d new speakers.\n", "- Performance specifically for Kilian.\n", "\n", "A reasonable strategy:\n", "\n", "1. Keep the speaker-independent split from part (1) for the 100 speakers.\n", "   - Train, validation, and test speakers remain disjoint.\n", "\n", "2. Create a separate split for Kilian\u2019s data, for example:\n", "   - Kilian-train: 70% of his recordings (or 3 of 5 days).\n", "   - Kilian-val:   15% (or 1 day).\n", "   - Kilian-test:  15% (or 1 day).\n", "\n", "3. Training:\n", "   - Train on the union of:\n", "     \u2022 Original training speakers (all their recordings), and\n", "     \u2022 Kilian-train recordings.\n", "   - During development, monitor two validation sets:\n", "     \u2022 Original validation speakers \u2192 general performance.\n", "     \u2022 Kilian-val \u2192 performance on this specific user.\n", "\n", "4. Final evaluation:\n", "   - Report two test metrics:\n", "     \u2022 Original test speakers \u2192 speaker-independent generalization.\n", "     \u2022 Kilian-test \u2192 personalized performance for Kilian.\n", "\n", "This setup both preserves a clean evaluation of generalization to new speakers and gives the model extra data to perform well on Kilian.\n", "\"\"\"\n"]}, {"cell_type": "markdown", "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8", "metadata": {}, "source": ["### Problem 2: K-Nearest Neighbors\n", "\n", "1. **1-NN Classification:** Given dataset:\n", "\n", "   Positive: (1,2), (1,4), (5,4)\n", "\n", "   Negative: (3,1), (3,2)\n", "\n", "   Plot the 1-NN decision boundary and classify new points visually.\n", "\n", "2. **Feature Scaling:** Consider dataset:\n", "\n", "   Positive: (100,2), (100,4), (500,4)\n", "\n", "   Negative: (300,1), (300,2)\n", "\n", "   What would the 1-NN classify point (500,1) as **before and after scaling** to [0,1] per feature?\n", "\n", "3. **Handling Missing Values:** How can you modify K-NN to handle missing features in a test point?\n", "\n", "4. **High-dimensional Data:** Why can K-NN still work well for images even with thousands of pixels?\n"]}, {"cell_type": "code", "execution_count": null, "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8", "metadata": {}, "outputs": [], "source": ["\"\"\"Problem 2 \u2013 K-Nearest Neighbors\n", "\n", "2.1 1-NN decision boundary\n", "\n", "Positive points: (1,2), (1,4), (5,4)\n", "Negative points: (3,1), (3,2)\n", "\n", "The 1-NN classifier labels each location in the plane by the class of the single closest training point under Euclidean distance. The decision boundary is formed by the perpendicular bisectors between points with different labels, restricted to their Voronoi cells.\n", "\n", "Below, I visualize the decision regions by sampling a grid of points, assigning each grid point to its nearest neighbor, and plotting the result.\n", "\n", "2.2 Effect of feature scaling\n", "\n", "We compare 1-NN predictions for the query (500, 1) before and after scaling features to [0, 1].\n", "\n", "Before scaling:\n", "\n", "Distances to positives:\n", "- d((500,1),(100,2))  \u2248 400.00\n", "- d((500,1),(100,4))  \u2248 400.01\n", "- d((500,1),(500,4))  = 3\n", "\n", "Distances to negatives:\n", "- d((500,1),(300,1))  = 200\n", "- d((500,1),(300,2))  \u2248 200.00\n", "\n", "The nearest neighbor is (500,4), a positive point \u2192 prediction: POSITIVE.\n", "\n", "After min\u2013max scaling using the training data:\n", "\n", "For feature x:\n", "- min_x = 100, max_x = 500 \u2192 x' = (x \u2212 100) / 400\n", "\n", "For feature y:\n", "- min_y = 1, max_y = 4   \u2192 y' = (y \u2212 1) / 3\n", "\n", "The scaled query becomes (1.0, 0.0). Its nearest neighbor becomes (300,1), which is negative \u2192 prediction: NEGATIVE.\n", "\n", "This shows that feature scaling can completely change the nearest neighbor when one feature has a much larger range than the other.\n", "\n", "2.3 Handling missing values in K-NN\n", "\n", "If some features of a test point are missing, one simple modification is:\n", "\n", "- When computing the distance between a test point x and a training point x_i, only use the coordinates where x is observed (and x_i has a value).\n", "- Optionally normalize the distance by the number of used coordinates so distances remain comparable.\n", "\n", "Another common option is to impute missing features (e.g., mean or median per feature) and then run standard K-NN.\n", "\n", "2.4 Why K-NN can work for images (high-dimensional data)\n", "\n", "Images have thousands of pixel features, so the ambient dimension is high. However, natural images occupy a much lower-dimensional manifold: they have structure, local correlations, and class-dependent patterns. Images from the same class tend to cluster together in pixel or embedding space.\n", "\n", "With enough training examples, and especially when using good feature representations (e.g., embeddings from a neural network), the nearest neighbors of an image are often visually and semantically similar. Therefore, K-NN can still work well in these high-dimensional but structured spaces.\n", "\"\"\"\n", "\n", "# Visualization code for Problem 2.1\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# training points\n", "pos = np.array([[1, 2], [1, 4], [5, 4]])\n", "neg = np.array([[3, 1], [3, 2]])\n", "\n", "X = np.vstack([pos, neg])\n", "y = np.array([1, 1, 1, -1, -1])\n", "\n", "# grid over the plane\n", "xx, yy = np.meshgrid(np.linspace(0, 6, 200), np.linspace(0, 5, 200))\n", "grid = np.c_[xx.ravel(), yy.ravel()]\n", "\n", "# 1-NN predictions\n", "pred = []\n", "for g in grid:\n", "    dists = np.linalg.norm(X - g, axis=1)\n", "    nn_idx = np.argmin(dists)\n", "    pred.append(y[nn_idx])\n", "pred = np.array(pred).reshape(xx.shape)\n", "\n", "plt.figure()\n", "plt.contourf(xx, yy, pred, alpha=0.2)\n", "plt.scatter(pos[:, 0], pos[:, 1], marker='o', label='positive')\n", "plt.scatter(neg[:, 0], neg[:, 1], marker='x', label='negative')\n", "plt.xlabel(\"x1\")\n", "plt.ylabel(\"x2\")\n", "plt.title(\"Problem 2.1 \u2013 1-NN Decision Boundary\")\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7", "metadata": {}, "source": ["### Problem 3: Part 1\n", "\n", "You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.\n", "\n", "1. Your co-worker suggests evaluating $h(x) = sign(w \\cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?\n", "2. Why is there no need to compute training error explicitly for the Perceptron algorithm?"]}, {"cell_type": "code", "execution_count": null, "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f", "metadata": {}, "outputs": [], "source": ["\"\"\"Problem 3 \u2013 Part 1: Perceptron training vs test error\n", "\n", "We are given a trained Perceptron with weight vector w and two datasets: a training set D_TR and a test set D_TE.\n", "\n", "(1) Using h(x) = sign(w \u00b7 x) to compare training and test error\n", "\n", "We can apply h(x) = sign(w \u00b7 x) to every example in both D_TR and D_TE:\n", "\n", "- For each (x, y) in D_TR:\n", "  \u2022 Predict y_hat = sign(w \u00b7 x).\n", "  \u2022 Count misclassifications where y_hat != y.\n", "  \u2022 Training error = (number of misclassified training examples) / |D_TR|.\n", "\n", "- For each (x, y) in D_TE:\n", "  \u2022 Predict y_hat = sign(w \u00b7 x).\n", "  \u2022 Test error = (number of misclassified test examples) / |D_TE|.\n", "\n", "Comparing these two error rates tells us whether the model is overfitting (e.g., very low training error but noticeably higher test error).\n", "\n", "(2) Why we do not need to explicitly compute training error for the Perceptron\n", "\n", "For linearly separable data, the Perceptron algorithm updates w only when it misclassifies a training point. It stops when it can make a full pass through the training set with no misclassifications.\n", "\n", "At convergence:\n", "- Every training example satisfies y (w \u00b7 x) > 0.\n", "- Therefore, the training error is exactly 0.\n", "\n", "Because of this property, we do not need to run a separate pass to compute the training error: the fact that the algorithm has converged already guarantees zero training error on D_TR (assuming separability).\n", "\"\"\"\n"]}, {"cell_type": "markdown", "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc", "metadata": {}, "source": ["### Problem 3: Two-point 2D Dataset (Part 2)\n", "\n", "Run the Perceptron algorithm **by hand or in code** on the following data:\n", "\n", "1. Positive class: (10, -2)\n", "2. Negative class: (12, 2)\n", "\n", "Start with $w_0 = (0, 0)$ and a learning rate of 1.\n", "\n", "- Compute how many updates are required until convergence.\n", "- Write down the sequence of $w_i$ vectors."]}, {"cell_type": "code", "execution_count": null, "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625", "metadata": {}, "outputs": [], "source": ["\"\"\"Problem 3 \u2013 Two-point 2D Dataset (Part 2)\n", "\n", "Data:\n", "- Positive point x_pos = (10, -2) with label y = +1\n", "- Negative point x_neg = (12,  2) with label y = -1\n", "\n", "We run the Perceptron with learning rate 1 and initial weight\n", "w0 = (0, 0),\n", "and we iterate over the data in the fixed order: first x_pos, then x_neg.\n", "\n", "Updates:\n", "\n", "w0 = (0, 0)\n", "\n", "1st pass:\n", "- x_pos: y (w0 \u00b7 x_pos) = 1 * (0) = 0 \u2192 misclassified\n", "  w1 = w0 + y x_pos = (0, 0) + (10, -2) = (10, -2)\n", "\n", "- x_neg: w1 \u00b7 x_neg = 10*12 + (-2)*2 = 120 - 4 = 116, y = -1\n", "  y (w1 \u00b7 x_neg) = -1 * 116 = -116 \u2264 0 \u2192 misclassified\n", "  w2 = w1 + y x_neg = (10, -2) + (-12, -2) = (-2, -4)\n", "\n", "2nd pass:\n", "- x_pos: w2 \u00b7 x_pos = (-2)*10 + (-4)*(-2) = -20 + 8 = -12 \u2192 misclassified\n", "  w3 = w2 + x_pos = (-2, -4) + (10, -2) = (8, -6)\n", "\n", "- x_neg: w3 \u00b7 x_neg = 8*12 + (-6)*2 = 96 - 12 = 84, y = -1\n", "  y (w3 \u00b7 x_neg) = -84 \u2264 0 \u2192 misclassified\n", "  w4 = w3 + (-1) x_neg = (8, -6) + (-12, -2) = (-4, -8)\n", "\n", "3rd pass:\n", "- x_pos misclassified \u2192 w5 = w4 + x_pos = (-4, -8) + (10, -2) = (6, -10)\n", "- x_neg misclassified \u2192 w6 = w5 + (-1) x_neg = (6, -10) + (-12, -2) = (-6, -12)\n", "\n", "4th pass:\n", "- x_pos misclassified \u2192 w7 = w6 + x_pos = (-6, -12) + (10, -2) = (4, -14)\n", "- x_neg misclassified \u2192 w8 = w7 + (-1) x_neg = (4, -14) + (-12, -2) = (-8, -16)\n", "\n", "5th pass:\n", "- x_pos: w8 \u00b7 x_pos = (-8)*10 + (-16)*(-2) = -80 + 32 = -48 \u2192 misclassified\n", "  w9 = w8 + x_pos = (-8, -16) + (10, -2) = (2, -18)\n", "\n", "Check:\n", "- x_neg: w9 \u00b7 x_neg = 2*12 + (-18)*2 = 24 - 36 = -12, y = -1\n", "  y (w9 \u00b7 x_neg) = -1 * (-12) = 12 > 0 \u2192 correctly classified.\n", "- x_pos: w9 \u00b7 x_pos = 2*10 + (-18)*(-2) = 20 + 36 = 56 > 0 \u2192 correctly classified.\n", "\n", "At w9 = (2, -18), both points are correctly classified, so the algorithm stops.\n", "\n", "Number of updates: 9\n", "\n", "Sequence of weight vectors:\n", "w0 = (0, 0)\n", "w1 = (10, -2)\n", "w2 = (-2, -4)\n", "w3 = (8, -6)\n", "w4 = (-4, -8)\n", "w5 = (6, -10)\n", "w6 = (-6, -12)\n", "w7 = (4, -14)\n", "w8 = (-8, -16)\n", "w9 = (2, -18)\n", "\"\"\"\n", "\n", "# Optional: small check with code\n", "import numpy as np\n", "\n", "x_pos = np.array([10.0, -2.0])\n", "x_neg = np.array([12.0,  2.0])\n", "points = [x_pos, x_neg]\n", "labels = [1, -1]\n", "\n", "w = np.array([0.0, 0.0])\n", "history = [w.copy()]\n", "\n", "for _ in range(20):  # safety limit\n", "    changed = False\n", "    for x, y in zip(points, labels):\n", "        if y * np.dot(w, x) <= 0:\n", "            w = w + y * x\n", "            history.append(w.copy())\n", "            changed = True\n", "    if not changed:\n", "        break\n", "\n", "print(\"Perceptron weight sequence:\")\n", "for i, w_i in enumerate(history):\n", "    print(f\"w{i} =\", tuple(w_i))\n"]}, {"cell_type": "markdown", "id": "3ba29c20-59b0-456f-994e-05897175596e", "metadata": {}, "source": ["### Problem 4: Reconstructing the Weight Vector\n", "\n", "Given the log of Perceptron updates:\n", "\n", "| x | y | count |\n", "|---|---|--------|\n", "| (0, 0, 0, 0, 4) | +1 | 2 |\n", "| (0, 0, 6, 5, 0) | +1 | 1 |\n", "| (3, 0, 0, 0, 0) | -1 | 1 |\n", "| (0, 9, 3, 6, 0) | -1 | 1 |\n", "| (0, 1, 0, 2, 5) | -1 | 1 |\n", "\n", "Assume learning rate = 1 and initial weight $w_0 = (0, 0, 0, 0, 0)$.\n", "\n", "Compute the final weight vector after all updates."]}, {"cell_type": "code", "execution_count": null, "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079", "metadata": {}, "outputs": [], "source": ["\"\"\"Problem 4 \u2013 Reconstructing the Weight Vector\n", "\n", "We start with initial weight\n", "w0 = (0, 0, 0, 0, 0)\n", "and learning rate 1.\n", "\n", "Each row in the table records an example x, its label y, and how many times this example caused an update. Each update is:\n", "    w \u2190 w + y x\n", "So if an example is used 'count' times, its total contribution is:\n", "    \u0394w = count \u00b7 y \u00b7 x\n", "\n", "Row 1: x = (0, 0, 0, 0, 4), y = +1, count = 2\n", "\u0394w1 = 2 \u00b7 (+1) \u00b7 (0, 0, 0, 0, 4) = (0, 0, 0, 0, 8)\n", "w1 = w0 + \u0394w1 = (0, 0, 0, 0, 8)\n", "\n", "Row 2: x = (0, 0, 6, 5, 0), y = +1, count = 1\n", "\u0394w2 = 1 \u00b7 (+1) \u00b7 (0, 0, 6, 5, 0) = (0, 0, 6, 5, 0)\n", "w2 = w1 + \u0394w2 = (0, 0, 6, 5, 8)\n", "\n", "Row 3: x = (3, 0, 0, 0, 0), y = \u22121, count = 1\n", "\u0394w3 = 1 \u00b7 (\u22121) \u00b7 (3, 0, 0, 0, 0) = (\u22123, 0, 0, 0, 0)\n", "w3 = w2 + \u0394w3 = (\u22123, 0, 6, 5, 8)\n", "\n", "Row 4: x = (0, 9, 3, 6, 0), y = \u22121, count = 1\n", "\u0394w4 = 1 \u00b7 (\u22121) \u00b7 (0, 9, 3, 6, 0) = (0, \u22129, \u22123, \u22126, 0)\n", "w4 = w3 + \u0394w4 = (\u22123, \u22129, 3, \u22121, 8)\n", "\n", "Row 5: x = (0, 1, 0, 2, 5), y = \u22121, count = 1\n", "\u0394w5 = 1 \u00b7 (\u22121) \u00b7 (0, 1, 0, 2, 5) = (0, \u22121, 0, \u22122, \u22125)\n", "w5 = w4 + \u0394w5 = (\u22123, \u221210, 3, \u22123, 3)\n", "\n", "Therefore, the final Perceptron weight vector is:\n", "w_final = (\u22123, \u221210, 3, \u22123, 3)\n", "\"\"\"\n", "\n", "# Optional: verify with code\n", "import numpy as np\n", "\n", "w = np.zeros(5, dtype=float)\n", "\n", "rows = [\n", "    ((0, 0, 0, 0, 4),  1, 2),\n", "    ((0, 0, 6, 5, 0),  1, 1),\n", "    ((3, 0, 0, 0, 0), -1, 1),\n", "    ((0, 9, 3, 6, 0), -1, 1),\n", "    ((0, 1, 0, 2, 5), -1, 1),\n", "]\n", "\n", "for x, y, count in rows:\n", "    x = np.array(x, dtype=float)\n", "    w += count * y * x\n", "\n", "print(\"Computed final weight vector:\", w)\n"]}, {"cell_type": "markdown", "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0", "metadata": {}, "source": ["### Problem 5: Visualizing Perceptron Convergence\n", "\n", "Implement a Perceptron on a small 2D dataset with positive and negative examples.\n", "\n", "- Plot the data points.\n", "- After each update, visualize the decision boundary.\n", "- Show how it converges to a stable separator."]}, {"cell_type": "code", "execution_count": null, "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3", "metadata": {}, "outputs": [], "source": ["\"\"\"Problem 5 \u2013 Visualizing Perceptron Convergence\n", "\n", "In this problem, I construct a simple 2D, linearly separable dataset, train a Perceptron with a bias term, and plot the decision boundary after each update. This shows how the separating hyperplane moves as the algorithm converges.\n", "\"\"\"\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# Simple linearly separable 2D dataset\n", "X_pos = np.array([[2, 3],\n", "                  [3, 4],\n", "                  [4, 3]])\n", "X_neg = np.array([[2, 1],\n", "                  [3, 0],\n", "                  [4, 1]])\n", "\n", "X = np.vstack([X_pos, X_neg])\n", "y = np.hstack([np.ones(len(X_pos)), -np.ones(len(X_neg))])\n", "\n", "# Add bias term: x' = [x1, x2, 1]\n", "X_aug = np.hstack([X, np.ones((X.shape[0], 1))])\n", "\n", "# Initialize weights (including bias)\n", "w = np.zeros(3)\n", "history = [w.copy()]\n", "\n", "max_epochs = 20\n", "for epoch in range(max_epochs):\n", "    changed = False\n", "    for i in range(len(X_aug)):\n", "        if y[i] * (X_aug[i] @ w) <= 0:\n", "            # Perceptron update\n", "            w = w + y[i] * X_aug[i]\n", "            history.append(w.copy())\n", "            changed = True\n", "    if not changed:\n", "        break\n", "\n", "print(\"Total number of updates:\", len(history) - 1)\n", "print(\"Final weight vector:\", history[-1])\n", "\n", "# Visualization of decision boundary after each update\n", "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n", "\n", "for t, w_t in enumerate(history):\n", "    plt.figure()\n", "    plt.scatter(X_pos[:, 0], X_pos[:, 1], marker='o', label='positive')\n", "    plt.scatter(X_neg[:, 0], X_neg[:, 1], marker='x', label='negative')\n", "\n", "    # decision boundary: w1 * x1 + w2 * x2 + w3 = 0 \u21d2 x2 = -(w1 * x1 + w3) / w2\n", "    if abs(w_t[1]) > 1e-6:\n", "        xs = np.linspace(x_min, x_max, 100)\n", "        ys = -(w_t[0] * xs + w_t[2]) / w_t[1]\n", "        plt.plot(xs, ys, label=f'after update {t}')\n", "    else:\n", "        # Vertical line if w2 is zero (rare in this toy example)\n", "        xs = np.full(100, -w_t[2] / (w_t[0] + 1e-8))\n", "        ys = np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100)\n", "        plt.plot(xs, ys, label=f'after update {t}')\n", "\n", "    plt.title(f\"Perceptron decision boundary \u2013 update {t}\")\n", "    plt.xlabel(\"x1\")\n", "    plt.ylabel(\"x2\")\n", "    plt.legend()\n", "    plt.show()\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.13.9"}}, "nbformat": 4, "nbformat_minor": 5}